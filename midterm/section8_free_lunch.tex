Consider any hypothesis class $\mathcal{H}$. 
\begin{parts}
\part[10] (10 pts) Recall the no free lunch theorem: If the sample size $n$ and the feature space $\cal X$ satisfy $n<\frac{|\cal X|}{2}$ then,
for any learning algorithm (whose output is denoted by $h$) it must be that 
$$\mathbb{P}(R(h)\geq \frac{1}{8})\geq \frac{1}{7}.$$

Use the statement of the no free lunch theorem to prove that if the VC dimension of $\mathcal{H}$ is infinite, then $\mathcal{H}$ is not PAC learnable under realizability.

\part[5] (5 pts) Prove that if  $\mathcal{H}$ is not PAC learnable under the realizability assumption then it is not agnostically PAC learnable.
\end{parts}