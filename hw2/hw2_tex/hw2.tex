\section{Soft Support Vector Machine Theory [40]}
\label{prb:hw2::prob1}

Consider the primal problem for the soft support vector machine (soft SVM).  Where $y_i \in \{-1,1\}$ are the labels, $\mathbf{x}_i \in \mathbb{R}^p$, $i=1,\ldots,n$ are the features (features already include the bias term), $\xi_{i} \in \mathbb{R}^{+}$ are the slack variables.

\begin{equation}
\label{soft_svm}
\begin{aligned}
& \underset{\mathbf{w}, \xi_i}{\text{minimize}} && \frac{1}{2}||\mathbf{w}||_2^2 + C\sum_{i=1}^n\xi_i\\
& \text{subject to} && y_i(\mathbf{w}^T\mathbf{x}_i) \geq 1-\xi_i & i = 1,\ldots, n\\
& && \xi_i \geq 0 & i = 1,\ldots, n
\end{aligned}
\end{equation}

\begin{itemize}[(a)]
    \item (30 points) Show that the soft SVM problem can be written as a regularized Hinge Loss problem:
\end{itemize}
\begin{equation}
\label{regularized_hinge_loss}
\underset{\mathbf{w}, \xi_i}{\text{minimize}} \;\; \frac{1}{2}||\mathbf{w}||_2^2 + C \sum_{i=1}^n \max\left(0, 1 - y_{i} (\mathbf{w}^T\mathbf{x}_i) \right)
\end{equation}

\begin{itemize}[(b)]
\label{subgradient}
    \item (10 points) Find an expression for the subgradient of the regularized Hinge Loss problem from equation (\ref{regularized_hinge_loss}).
\end{itemize}


\newpage
\section{Implementation of the Soft SVM [60 points]}
\label{prb:hw2::prob2}

\begin{algorithm}[H]
\SetAlgoLined
\KwIn{Number of steps $T$, learning rate $\alpha$ and batch size $b$}
Initialize parameters $\mathbf{w}_{0}=0$, $b_{0}=0$, step $t=0$\;
 \For{$t \; in \; 1,\dots, T$}{
   Sample a batch $\mathbf{b}=\{(x_{i}, y_{i}) \}$ with $|\text{batch}|=b$\;
   Compute the subgradient $\hat{\nabla f}(w) = 1/b \sum_{i \in \mathbf{b}} \nabla f(x_{i}, y_{i})$ \;
   Update the parameters $\textbf{w}_{t+1}=\textbf{w}_{t+1}-\alpha \hat{\nabla f}(w)$\;
 }
 \caption{Gradient Descent algorithm}
\end{algorithm}

\begin{enumerate}[(a)]
\item (0 points) For the data provided by \textcolor{blue}{data.py}, write a function to prepare data that concatenates a constant to the original data $\mathbf{X} =[\mathbbm{1}|\mathbf{X}_{original}]$ so that the bias term is implicit in the weights $\mathbf{w}$, and change $y$ to $\{-1, 1\}$.

\item (5 points) Complete the code of the Soft SVM Loss method. This method includes both the Hinge Loss and the L2 regularization term. %It receives as input $\mathbf{X}$ of shape (n,d) and $y$ of shape (n,).

\item (5 points) Complete the code of the subgradient method of the Soft SVM Loss, that you derived in question (1.b). %the method takes as input $\mathbf{X}$ of shape (n,d) and $y$ of shape (n,).

\item (5 points) Complete the code of the predict method. % it takes as input $\mathbf{X}$ of shape (n,d) and outputs the predictions $\hat{y}$.

\item (10 points) Complete the sampling method which takes as input $\mathbf{X}$ and $y$, batch size, and returns a sampled batch $\mathbf{X}$.

\item (15 points) Complete the train method of the soft SVM using the gradient descent algorithm. The train method should keep track of the train and test SVM loss trajectories (regularization + hinge), the train and test accuracy trajectories and the computational time of each step.

\end{enumerate}

Train your soft SVM classifiers on the data from \textcolor{blue}{data.py}.  Report the following plots in your pdf \textcolor{blue}{full gradient,  stochastic gradient (batch size of 1) and mini batch} of batch size 128.

\begin{enumerate}[(a)]\setcounter{enumi}{6}
\item (5 points) Plot the train and test \textbf{losses} trajectories against iterations.

\item (5 points) Plot the train and test \textbf{losses} trajectories against time.

\item (5 points) Plot the train and test \textbf{accuracy} trajectories against iterations.

\item (5 points) Plot the train and test \textbf{accuracy} trajectories against time.
\end{enumerate}
